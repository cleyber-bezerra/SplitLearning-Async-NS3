{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPyF7f1yaQUZdtAryCBxohZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cleyber-bezerra/SplitLearning-Async-NS3/blob/main/AsyncSL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importando Biblioteca Python"
      ],
      "metadata": {
        "id": "f1MxHmLN6VYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "T9VrmHNG6atc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importando Base de Dados (Data set)"
      ],
      "metadata": {
        "id": "6fzcUwTS6oSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo as transformações para os dados de validação\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "# Carregando o dataset de validação (MNIST neste caso)\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "# Criando o DataLoader para validação\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVadSpcr6it3",
        "outputId": "37753be8-a652-47a9-f957-6afd7d41798b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 16389328.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 485831.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3855964.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 9270330.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ClientNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ClientNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        return x\n",
        "\n",
        "class ServerNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ServerNet, self).__init__()\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "26EOu2zR60P0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#INTRODUCAO DE FALHAS DE TRANSMISSAO\n",
        "def introduce_transmission_errors(tensor, error_rate):\n",
        "    if error_rate == 0:\n",
        "        return tensor\n",
        "    device = tensor.device  # Obter o dispositivo do tensor original\n",
        "    mask = torch.rand(tensor.size(), device=device) > error_rate  # Criar a máscara no mesmo dispositivo\n",
        "    return tensor * mask\n",
        "\n",
        "#INTRODUCAO DE LATENCIA NAS TRANSMISSOES\n",
        "def introduce_latency(latencies, delta_t):\n",
        "    latency = random.choice(latencies)\n",
        "    if latency > delta_t:\n",
        "        return None, latency  # Consider as timeout\n",
        "    else:\n",
        "        return latency, latency  # Successful transmission"
      ],
      "metadata": {
        "id": "LP07LCvY7B7l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(client_models, server_model, train_loader, criterion, optimizer, latencies, delta_t, error_rate, device):\n",
        "    for client_model in client_models:\n",
        "        client_model.train()\n",
        "    server_model.train()\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    latencies_record = []\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Simulating latency\n",
        "        latency, latency_val = introduce_latency(latencies, delta_t)\n",
        "        latencies_record.append(latency_val)\n",
        "        if latency is None:\n",
        "            continue  # Simulate timeout (loss of message)\n",
        "\n",
        "        # Forward pass with each client model\n",
        "        outputs = sum(server_model(client_model(data)) for client_model in client_models) / len(client_models)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    train_loss /= total\n",
        "    accuracy = 100. * correct / total\n",
        "    return train_loss, accuracy, latencies_record"
      ],
      "metadata": {
        "id": "GhKX-pU97XiS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(client_models, server_model, val_loader, criterion, latencies, delta_t, error_rate, device):\n",
        "    for client_model in client_models:\n",
        "        client_model.eval()\n",
        "    server_model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    latencies_record = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Simulating latency\n",
        "            latency, latency_val = introduce_latency(latencies, delta_t)\n",
        "            latencies_record.append(latency_val)\n",
        "            if latency is None:\n",
        "                continue  # Simulate timeout (loss of message)\n",
        "\n",
        "            outputs = sum(server_model(client_model(data)) for client_model in client_models) / len(client_models)\n",
        "            loss = criterion(outputs, target)\n",
        "\n",
        "            val_loss += loss.item() * data.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    val_loss /= total\n",
        "    accuracy = 100. * correct / total\n",
        "    return val_loss, accuracy, latencies_record"
      ],
      "metadata": {
        "id": "miICIjgh7b5m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_based_quantization(X):\n",
        "    median = np.median(np.abs(X))\n",
        "    for ebit in range(3, 6):\n",
        "        max_value = (2 ** (2 ** ebit - 1)) - 1\n",
        "        min_value = 1\n",
        "        max_bias = np.log2(min_value/median)\n",
        "        min_bias = np.log2(median/max_value)\n",
        "        for bias in np.arange(min_bias, max_bias, 0.01):\n",
        "            overflow = np.sum(X > max_value * 2 ** bias) / len(X)\n",
        "            underflow = np.sum(X < min_value * 2 ** bias) / len(X)\n",
        "            clip = overflow + underflow\n",
        "            if clip < 0.01:\n",
        "                return bias\n",
        "    return None"
      ],
      "metadata": {
        "id": "UvAQASqd7ikn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AsynchronousSplitLearning:\n",
        "    def __init__(self, client_models, server_model, num_epoch, num_batch, K, lthred):\n",
        "        self.state = 'A'\n",
        "        self.client_models = client_models\n",
        "        self.server_model = server_model\n",
        "        self.num_epoch = num_epoch\n",
        "        self.num_batch = num_batch\n",
        "        self.K = K\n",
        "        self.lthred = lthred\n",
        "        self.total_loss = 0\n",
        "\n",
        "    def split_forward(self, state, data, target, criterion):\n",
        "        if state == 'C':\n",
        "            act, y_star = None, None\n",
        "        else:\n",
        "            act = sum(client_model(data) for client_model in self.client_models) / len(self.client_models)\n",
        "            y_star = target\n",
        "        outputs = self.server_model(act)\n",
        "        loss = criterion(outputs, target)\n",
        "        return loss\n",
        "\n",
        "    def split_backward(self, state, loss, optimizer):\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    def update_state(self, total_loss):\n",
        "        last_update_loss = total_loss / (self.num_batch * self.K)\n",
        "        delta_loss = last_update_loss - (total_loss / (self.num_batch * self.K))\n",
        "        if delta_loss <= self.lthred:\n",
        "            self.state = 'A'\n",
        "        else:\n",
        "            self.state = 'B' if self.state == 'A' else 'C'\n",
        "        return self.state\n",
        "\n",
        "    def train(self, train_loader, criterion, optimizer, latencies, delta_t, error_rate, device):\n",
        "        for epoch in range(1, self.num_epoch + 1):\n",
        "            total_loss = 0\n",
        "            for client in range(1, self.K + 1):\n",
        "                for batch_idx, (data, target) in enumerate(train_loader):\n",
        "                    data, target = data.to(device), target.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    latency, latency_val = introduce_latency(latencies, delta_t)\n",
        "                    if latency is None:\n",
        "                        continue\n",
        "\n",
        "                    loss = self.split_forward(self.state, data, target, criterion)\n",
        "                    total_loss += loss.item()\n",
        "                    self.split_backward(self.state, loss, optimizer)\n",
        "            self.state = self.update_state(total_loss)"
      ],
      "metadata": {
        "id": "M8z8_IA77rhK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be-dfchf59s3"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "\n",
        "client_models = [ClientNet().to(device) for _ in range(3)]\n",
        "server_model = ServerNet().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() #FUNCAO DE PERDA\n",
        "optimizer = optim.SGD(\n",
        "    [param for client_model in client_models for param in client_model.parameters()] + list(server_model.parameters()),\n",
        "    lr=0.01,                      #Otimizador\n",
        "    momentum=0.9\n",
        ")\n",
        "\n",
        "latencies = [1, 2, 3, 4, 5]\n",
        "delta_t = 3  # Timeout threshold\n",
        "error_rates = [0.0, 0.25, 0.50, 0.75]\n",
        "\n",
        "num_epochs = 4  # the article 200\n",
        "train_size = 5  # the article 10000\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "all_train_latencies = []\n",
        "all_val_latencies = []\n",
        "biases = []\n",
        "\n",
        "for error_rate in error_rates:\n",
        "    print(f\"Training with error rate: {error_rate * 100}%\")\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        if biases.append(search_based_quantization(np.random.randn(train_size))):\n",
        "            asl = AsynchronousSplitLearning(client_models, server_model, num_epoch=num_epochs, num_batch=len(train_loader), K=3, lthred=0.01)\n",
        "            asl.train(train_loader, criterion, optimizer, latencies, delta_t, error_rate, device)\n",
        "\n",
        "            train_loss, train_accuracy, train_latencies = train(client_models, server_model, train_loader, criterion, optimizer, latencies, delta_t, error_rate, device)\n",
        "            val_loss, val_accuracy, val_latencies = validate(client_models, server_model, train_loader, criterion, latencies, delta_t, error_rate, device)\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "            train_accuracies.append(train_accuracy)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            all_train_latencies.extend(train_latencies)\n",
        "            all_val_latencies.extend(val_latencies)\n",
        "\n",
        "            print(f'Epoch {epoch}/{num_epochs} - '\n",
        "                  f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}% - '\n",
        "                  f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "            print(f'TRAIN ASSYNC')\n",
        "        else:\n",
        "            train_loss, train_accuracy, train_latencies = train(client_models, server_model, train_loader, criterion, optimizer, latencies, delta_t, error_rate, device)\n",
        "            val_loss, val_accuracy, val_latencies = validate(client_models, server_model, train_loader, criterion, latencies, delta_t, error_rate, device)\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "            train_accuracies.append(train_accuracy)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            all_train_latencies.extend(train_latencies)\n",
        "            all_val_latencies.extend(val_latencies)\n",
        "\n",
        "            print(f'Epoch {epoch}/{num_epochs} - '\n",
        "                  f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}% - '\n",
        "                  f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "            print(f'TRAIN SYNC')\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "   # Plotting latencies\n",
        "    plt.figure(figsize=(num_epochs, 5))\n",
        "    plt.hist(all_train_latencies, bins=[1, 2, 3, 4, 5, 6], edgecolor='black')\n",
        "    plt.axvline(x=delta_t, color='r', linestyle='--', label=f'Timeout Threshold: {delta_t} ms')\n",
        "    plt.title('Latency Distribution During Training')\n",
        "    plt.xlabel('Latency (ms)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    print(num_epochs)\n",
        "    print(len(train_losses))\n",
        "    print(len(val_losses))\n",
        ""
      ],
      "metadata": {
        "id": "q3eEtuIA79wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    plt.figure(figsize=(num_epochs, 5))\n",
        "\n",
        "    # Plotting the losse\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
        "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Train and Validation Loss')\n",
        "    plt.legend()\n",
        "    # Plotting the accuracies\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train Accuracy')\n",
        "    plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Train and Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "HPFsImSq8BK3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}